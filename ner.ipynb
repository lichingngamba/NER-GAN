{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for the NER data\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.data = pd.read_csv(filepath, encoding='ISO-8859-1').fillna(method='ffill')\n",
    "        \n",
    "        # Create a list of unique words and tags\n",
    "        self.words = list(set(self.data[\"Word\"].values))\n",
    "        self.tags = list(set(self.data[\"Tag\"].values))\n",
    "        \n",
    "        # Create dictionaries for mapping words and tags to integers\n",
    "        self.word2idx = {w: i+1 for i, w in enumerate(self.words)}\n",
    "        self.tag2idx = {t: i for i, t in enumerate(self.tags)}\n",
    "        \n",
    "        self.sentences = self._get_sentences()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        words = [word[0] for word in sentence]\n",
    "        tags = [word[1] for word in sentence]\n",
    "        \n",
    "        # Convert words and tags to numerical values using the dictionaries\n",
    "        x = [self.word2idx[w] for w in words]\n",
    "        y = [self.tag2idx[t] for t in tags]\n",
    "        \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "    \n",
    "    def _get_sentences(self):\n",
    "        # Group the data by sentence number\n",
    "        grouped = self.data.groupby(\"Sentence #\")\n",
    "        sentences = []\n",
    "        for _, group in grouped:\n",
    "            words = group[\"Word\"].values.tolist()\n",
    "            tags = group[\"Tag\"].values.tolist()\n",
    "            sentence = list(zip(words, tags))\n",
    "            sentences.append(sentence)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple LSTM-based NER model\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ltensuba\\AppData\\Local\\Temp\\ipykernel_2200\\1895907770.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  self.data = pd.read_csv(filepath, encoding='ISO-8859-1').fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters and train the model\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 10\n",
    "\n",
    "dataset = NERDataset(r\"C:\\Python_code\\practise_code\\data\\ner_datasetreference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    y = [item[1] for item in batch]\n",
    "    x_lengths = [len(seq) for seq in x]\n",
    "    y_lengths = [len(seq) for seq in y]\n",
    "    \n",
    "    # Pad the sequences to the same length\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
    "    \n",
    "    # Create a mask to ignore padding values in the loss calculation\n",
    "    x_mask = torch.arange(x.size(1))[None, :] < torch.tensor(x_lengths)[:, None]\n",
    "    y_mask = torch.arange(y.size(1))[None, :] < torch.tensor(y_lengths)[:, None]\n",
    "    \n",
    "    return x, y, x_mask, y_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn = collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  158, 31773,  2901,  ...,     0,     0,     0],\n",
      "        [34342, 17737, 33672,  ...,     0,     0,     0],\n",
      "        [31201, 29435,  2901,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [19581, 33832, 33137,  ...,     0,     0,     0],\n",
      "        [ 7349, 21237, 16129,  ...,     0,     0,     0],\n",
      "        [21464, 11071,  1294,  ..., 10624, 23541, 26852]]) tensor([[13, 14, 13,  ...,  0,  0,  0],\n",
      "        [ 1, 13, 13,  ...,  0,  0,  0],\n",
      "        [ 1,  3, 13,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [13, 13, 13,  ...,  0,  0,  0],\n",
      "        [ 1,  3, 13,  ...,  0,  0,  0],\n",
      "        [11, 13, 13,  ...,  1, 13, 13]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0],batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERModel(len(dataset.words) + 1, EMBEDDING_DIM, HIDDEN_DIM, len(dataset.tags))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1389, Val Loss: 0.1229\n",
      "Epoch 2, Train Loss: 0.1176, Val Loss: 0.1246\n",
      "Epoch 3, Train Loss: 0.1183, Val Loss: 0.1250\n",
      "Epoch 4, Train Loss: 0.1154, Val Loss: 0.1227\n",
      "Epoch 5, Train Loss: 0.1145, Val Loss: 0.1252\n",
      "Epoch 6, Train Loss: 0.1140, Val Loss: 0.1274\n",
      "Epoch 7, Train Loss: 0.1144, Val Loss: 0.1236\n",
      "Epoch 8, Train Loss: 0.1135, Val Loss: 0.1238\n",
      "Epoch 9, Train Loss: 0.1122, Val Loss: 0.1240\n",
      "Epoch 10, Train Loss: 0.1149, Val Loss: 0.1278\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, y, x_mask, y_mask = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output.view(-1, len(dataset.tags)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    train_loss /= len(train_dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y, x_mask, y_mask = batch\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1, len(dataset.tags)), y.view(-1))\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "        val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-eve', 'O', 'O', 'O', 'B-eve', 'B-eve', 'I-org', 'I-org', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is Imphal and is giving out a demo of the trained NER model. Bangalore Bangalore Here We Come\"\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Convert words to numerical values using the word2idx dictionary\n",
    "x = [dataset.word2idx.get(word, 0) for word in words]\n",
    "\n",
    "# Convert the numerical values to a tensor and add a batch dimension\n",
    "x = torch.tensor(x).unsqueeze(0)\n",
    "\n",
    "# Pass the tensor through the model to get the predicted tags\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "    _, predicted_tags = torch.max(output, dim=2)\n",
    "\n",
    "# Convert the predicted tags back to their corresponding tag labels using the idx2tag dictionary\n",
    "predicted_tags = predicted_tags.squeeze().tolist()\n",
    "predicted_labels = [dataset.tags[idx] for idx in predicted_tags]\n",
    "print(predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
